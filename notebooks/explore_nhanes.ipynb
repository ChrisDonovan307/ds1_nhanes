{"cells":[{"cell_type":"markdown","metadata":{"id":"pp8EptmjQn0_"},"source":["# Explore NHANES\n","\n","This is a first swing at exploring NHANES datasets. Currently just working with the 2021-2023 survey and lumping both days into a single dataset.\n","\n","An issue that needs to be fixed here is that the grams of PBPs consumed below are based on the total foods/recipes, but are not broken down into what proportion are actually PBPs. Need to add the recipe doc that translates recipes into constituent ingredients and proportions to get proper PBP consumption."]},{"cell_type":"markdown","metadata":{"id":"bmdmBS2SlvUz"},"source":["## Set Working Directory\n","\n","When you open a notebook, the default working directory will be the folder that notebook is in. We want it to be the top (root) directory of the project, `ds1_nhanes`.\n","\n","First, we need to mount our Google Drive, which contains the `ds1_nhanes` folder. The following chunk will mount the drive (if in Google Colab) and set the working directory to the root of the project folder."]},{"cell_type":"code","source":["import os\n","\n","try:\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  os.chdir('/content/drive/MyDrive/ds1_nhanes/')\n","except:\n","  from pathlib import Path\n","  os.chdir(Path(os.getcwd()).parent)\n","\n","print(os.getcwd())"],"metadata":{"id":"Iw_QuVuLEfpV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"571LTcODQn1E"},"source":["Bingo bongo, we're good to go."]},{"cell_type":"markdown","metadata":{"id":"8-Zj2PKG7NDN"},"source":["## Load Data and Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zx5mz_OZ6_L2"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import re"]},{"cell_type":"markdown","metadata":{"id":"QAjxN-yXQn1F"},"source":["The DR1IFF_L and DR2IFF_L datasets include daily food intake over two days for respondents identified by the SEQN or respondent sequence identifier. The surveys were conducted in waves and we aim to combine two waves of data survey responses from 2017-2020 and 2021-2023. Given the size of these datasets we tried to pull only the columns that are relevant for our analysis are read for both days and both years, however, usecols does not work on .XPT files."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a2Fz02bE7PAu"},"outputs":[],"source":["# Read in the four datasets with specified columns\n","dr1_17 = pd.read_sas('data/raw/nhanes_2017_2020/P_DR1IFF.xpt')\n","dr2_17 = pd.read_sas('data/raw/nhanes_2017_2020/P_DR2IFF.xpt')\n","dr1_21 = pd.read_sas('data/raw/nhanes_2021_2023/DR1IFF_L.xpt')\n","dr2_21 = pd.read_sas('data/raw/nhanes_2021_2023/DR2IFF_L.xpt')\n","\n","# Create a list of all the datasets\n","datasets = [dr1_21, dr2_21, dr1_17, dr2_17]\n","\n","# Define the relevant columns for this analysis\n","relevant_cols = ['SEQN', # response_sequence, unique identifier for each respondent\n","                 'WTDR2D', # weight_day_2_dietary, the weighting factor given to\n","                 # the second day depending on how many days the respondents reported\n","                 'WTDR2DPP', # different name in 2017 - 2020 data\n","                 'DR1IGRMS', # grams, total grams of food consumed, labeled DR1GRMS or DR2GRMS\n","                 # based on which day it was reported\n","                 'DR2IGRMS', # labeled DR1GRMS or DR2GRMS based on which day it was reported\n","                 'DR1IFDCD', # usda_food_code, food identifier\n","                 'DR2IFDCD'] # labeled DR1GRMS or DR2GRMS based on which day it was reported\n","for dataset in datasets:\n","  dataset.drop(columns=[col for col in dataset.columns if col not in relevant_cols], inplace=True)\n","\n","#Look up what columns are relevant and load only what is relevant, check class notes - LB\n","print(dr1_21.info())\n","print(dr2_21.info())\n","print(dr1_17.info())\n","print(dr2_17.info())\n","dr1_21.head()\n"]},{"cell_type":"markdown","metadata":{"id":"JVX4IziU5BJR"},"source":["## Explore Dietary Recall Data"]},{"cell_type":"markdown","metadata":{"id":"nZDggDAjQn1H"},"source":["Compare the dimensions of the df with the number of unique SEQN numbers (respondent ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ApGkuYMD5G59"},"outputs":[],"source":["# Compare rows to unique respondent IDs\n","# first get number of rows\n","rows = dr1_21.shape[0]\n","unique_seqns = dr1_21['SEQN'].nunique()\n","print(f\"{rows} rows and {unique_seqns} unique SEQN numbers in DR1.\")"]},{"cell_type":"markdown","metadata":{"id":"pW9P9alh5UO_"},"source":["There are far more rows than unique respondents. This is because for each respondent, there is one row for each individual food they consumed.\n","\n","Check out how many unique food codes there are:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hvTiw0nz5nLs"},"outputs":[],"source":["n_codes = dr1_21['DR1IFDCD'].nunique()\n","print(f\"There are {n_codes} unique food codes\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FkRylZaBYrov"},"outputs":[],"source":["# Check SEQNs between dr1 and dr2\n","\n","diff = set(dr1_17['SEQN']).difference(dr2_17['SEQN'])\n","print(f\"{len(diff)} SEQNs missing from DR2 that were in DR1 in the 2017 - 2020 wave\")\n","diff2 = set(dr2_17['SEQN']).difference(dr1_17['SEQN'])\n","print(f\"{len(diff2)} SEQNs missing from DR1 that were in DR2 in the 2017 - 2020 wave\")\n","\n","diff = set(dr1_21['SEQN']).difference(dr2_21['SEQN'])\n","print(f\"{len(diff)} SEQNs missing from DR2 that were in DR1 in the 2021 - 2023 wave\")\n","diff2 = set(dr2_21['SEQN']).difference(dr1_21['SEQN'])\n","print(f\"{len(diff2)} SEQNs missing from DR1 that were in DR2 in the 2021 - 2023 wave\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"V36g2ukMYrow"},"source":["For the 2017 - 2020 wave, there are 1804 respondants for day 1 that did not report in day 2, and two respondent who reported in day 2 but not day 1.\n","\n","For the 2021 - 2023 wave, there are 873 respondants for day 1 that did not report in day 2, and one respondent who reported in day 2 but not day 1.  people when we join.\n","\n","The two-day weights (WTDR2) was adjusted based on the day 1 weights (WTDR1) and further adjusting for additional non-response for the second recall, so we will drop the respondents that only respond in both days.\n"]},{"cell_type":"markdown","source":["## Join with FPED\n","\n","Join FPED to each of the four DR datasets. We have to do this and aggregate within each DR before we combine the DRs together, otherwise we get cartesian merges."],"metadata":{"id":"rnnM-XJQ-aaJ"}},{"cell_type":"code","source":["fped = pd.read_csv('data/miscellany/FPED_1720.csv')\n","fped.columns = fped.columns.str.lower()\n","fped.columns = fped.columns.str.replace(\" \", \"_\")\n","\n","fped.info()"],"metadata":{"id":"6RPxTNX-sWuN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Rename columns in DR datasets that are not consistent (FDCD, GRMS, and 2 day weights). This will make it easier map over the list of all datasets when we aggregate"],"metadata":{"id":"oqeZf1MD-6Oi"}},{"cell_type":"code","source":["# Function to rename columns across all datasets\n","# These are the ones we care about that differ between DR dataset\n","def rename_columns(df):\n","    new_columns = {}\n","    for col in df.columns:\n","        if re.search(r'FDCD$', col):\n","            new_columns[col] = 'food_code'\n","        elif re.search(r'GRMS$', col):\n","            new_columns[col] = 'grams'\n","        elif re.search(r'^WTDR2', col):\n","            new_columns[col] = 'weight_2d'\n","        else:\n","            new_columns[col] = col\n","\n","    df = df.rename(columns=new_columns)\n","    return df\n","\n","# Rename columns in each dataset\n","datasets_renamed = list(map(rename_columns, datasets))\n","\n","print(\"\\nRenamed Datasets:\\n\")\n","list(map(lambda df: df.info(), datasets_renamed))\n"],"metadata":{"id":"m8XdEBFd-yNA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Merge each DR with FPED"],"metadata":{"id":"ZN8VNv8b_Mvr"}},{"cell_type":"code","source":["# Map over our list of datasets and merge each out with FPED\n","def merge_with_fped(df):\n","    return df.merge(fped, left_on='food_code', right_on='foodcode', how='left')\n","\n","dfs_fped = list(map(merge_with_fped, datasets_renamed))\n","\n","print(\"\\nDatasets with FPED:\\n\")\n","list(map(lambda df: df.info(), dfs_fped))"],"metadata":{"id":"ogIT1-e4_L42"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can concat DR1 and DR2 together for each wave"],"metadata":{"id":"jDflkaVBIXPf"}},{"cell_type":"code","source":["# Combine first and second DF to make a df for 2021-2023\n","df_21 = pd.concat([dfs_fped[0], dfs_fped[1]], ignore_index=True)\n","df_21.info()\n","\n","# Combine second and third DF to make df for 2017-2020\n","df_17 = pd.concat([dfs_fped[2], dfs_fped[3]], ignore_index=True)\n","df_17.info()"],"metadata":{"id":"zdAKw_K7D4vP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Get food group totals for each food code for each person. Take grams, divide by 100, then multiply by every food group category."],"metadata":{"id":"PuQjMeqRH_b9"}},{"cell_type":"code","source":["# Put them back into a list\n","waves = [df_17, df_21]\n","\n","def get_food_group_totals(df):\n","    df.loc[:, 'f_total_(cup_eq)':'a_drinks_(no._of_drinks)'].multiply(df['grams']/100, axis=0)\n","    return df\n","\n","waves_fped = list(map(\n","    get_food_group_totals,\n","    waves\n","))\n","\n","waves_fped[0].head()"],"metadata":{"id":"qYhY4m-6IMZN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Group by SEQN and aggregate FPED variables"],"metadata":{"id":"ilY77aX3DiUC"}},{"cell_type":"code","source":["## Grouping by SEQN within each DR dataset\n","# Set aggregation functions so we don't have to do them all manually\n","# Everything except food code and description in FPED should be summedf\n","cols_to_sum = fped.columns[2:]\n","\n","# Set aggregation functions\n","aggs = {col: 'sum' for col in cols_to_sum}\n","aggs['SEQN'] = 'first'\n","aggs['weight_2d'] = 'unique'\n","aggs['grams'] = 'sum'\n","\n","# Aggregate each dataset\n","waves_grouped = list(map(\n","    lambda df:\n","        df.groupby('SEQN').agg(aggs),\n","    waves_fped\n","))\n","\n","waves_grouped[0].head()"],"metadata":{"id":"2f1sMTUc_YwA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we are down to 1 row per SEQN per wave, including both days of dietary recall.\n","\n","Finally we can concat both waves into a single df and divide 2day weights by 2\n"],"metadata":{"id":"btV1b2zUfMJx"}},{"cell_type":"code","source":["# Combine waves\n","df = pd.concat(waves_grouped, ignore_index=True)\n","\n","# Divide 2day weights by 2 since we are combining two waves\n","df['weight_2d'] = df['weight_2d'] / 2\n","\n","# Rearrange columns by bringing important ones to front\n","# Leaving in grams just as a check\n","front_cols = ['SEQN', 'weight_2d', 'grams']\n","cols = front_cols + [col for col in df.columns if col not in front_cols]\n","df = df[cols]\n","\n","df.info()"],"metadata":{"id":"MU5xjYYgQqdE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Make the SEQN column an integer and weight_2d a float with two decimal places\n","\n","df['SEQN'] = df['SEQN'].astype(int)\n","\n","df['weight_2d'] = df['weight_2d'].astype(float)\n","df['weight_2d'] = df['weight_2d'].round(2)\n","\n","df"],"metadata":{"id":"JxGXkufaVYqe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is hopefully a nice clean DF with one row per SEQN, both DR days, and two waves lumped together."],"metadata":{"id":"nFoXMX-ERZ7N"}},{"cell_type":"markdown","source":["## Add Demographics\n","\n","The NHANES dataset includes demographics for each respondent. The following steps merge key demographics with each respondent id number SEQN."],"metadata":{"id":"L3gqpMAajChM"}},{"cell_type":"markdown","metadata":{"id":"7RoMYTxVQn1J"},"source":["Load demographic data as xpt:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yWMYQm25Qn1J"},"outputs":[],"source":["demos_17 = pd.read_sas('data/raw/nhanes_2017_2020/P_DEMO.xpt')\n","demos_21 = pd.read_sas('data/raw/nhanes_2021_2023/DEMO_L.xpt')\n","\n","demos_17.info()\n","demos_21.info()\n","\n","# combine into a single dataset\n","demos = pd.concat([demos_17, demos_21], ignore_index=True)\n","demos.info()"]},{"cell_type":"markdown","metadata":{"id":"qXbnyY6fQn1J"},"source":["Take the columns SEQN, age, gender, race, education, and ratio of family income to poverty:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-9N3dgwKQn1J"},"outputs":[],"source":["demos = demos[['SEQN', 'RIAGENDR', 'RIDAGEYR', 'RIDRETH3', 'DMDEDUC2', 'INDFMPIR']]\n","# rename the demo columns\n","demos.columns = ['SEQN', 'gender', 'age', 'race', 'education', 'income_ratio']\n","demos.info()"]},{"cell_type":"markdown","metadata":{"id":"p6jkKxuzQn1M"},"source":["Merge demographics with our dietary intake data:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"boCk2J4ZQn1M"},"outputs":[],"source":["# check to see\n","\n","diff = set(df['SEQN']).difference(demos['SEQN'])\n","print(f\"{len(diff)} SEQNs missing from demographics that were in dietary intake\")\n","diff2 = set(demos['SEQN']).difference(df['SEQN'])\n","print(f\"{len(diff2)} SEQNs missing from dietary intake that were in demographics\")\n","\n","# Now join our demos with dietary intake data\n","df = df.merge(demos, on='SEQN', how='left')\n","df.info()"]},{"cell_type":"markdown","metadata":{"id":"fpvQstnMQn1N"},"source":["Recode demographic variables. Coding schemes are available at the NHANES website in the documentation beside each dataset. We are splitting the income to poverty ratio into quartiles."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4oRyX3nxQn1N"},"outputs":[],"source":["# Gender\n","df['gender'] = df['gender'].apply(lambda x: ('Female' if x == 2 else 'Male'))\n","\n","# Education\n","df['education'] = df['education'].apply(\n","  lambda x: (\n","    'Less than 9th grade' if x == 1\n","    else '9th to 11th grade' if x == 2\n","    else 'High school/GED' if x == 3\n","    else 'Some college or AA' if x == 4\n","    else 'College graduate or above' if x == 5\n","    else \"Don\\'t know\"\n","  )\n",")\n","\n","# Race\n","df['race'] = df['race'].apply(\n","  lambda x: (\n","    'Mexican American' if x == 1\n","    else 'Other Hispanic' if x == 2\n","    else 'White' if x == 3\n","    else 'Black' if x == 4\n","    else 'Asian' if x == 5\n","    else 'Other or Multi'\n","  )\n",")\n","\n","# Income to poverty ratio\n","df['income_ratio_qs'] = pd.qcut(\n","  x = df['income_ratio'],\n","  q = 4,\n","  duplicates='drop'\n",")\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"Xy2AiwxyQn1I"},"source":["## Identify plant-based protein eaters\n","\n","Get the column names which identify food codes as containing any amount of\n","legumes, nuts/seeds, or soy. Then identify the food codes that are PBPs and label as has PBPs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LXrvp3ACQn1I"},"outputs":[],"source":["# Keywords we will use to find column names\n","keywords = ['pf_legumes', 'pf_nutsds', 'pf_soy']\n","pbp_columns = [col for col in df.columns if any(keyword in col for keyword in keywords)]\n","print(pbp_columns)\n","\n","# Condition: Any of the selected columns has a value greater than 1\n","df['has_pbp'] = (df[pbp_columns] > 1).any(axis=1)\n","print(df.head())\n","\n","# Check that this worked\n","df[df['has_pbp'] == True][pbp_columns + ['has_pbp']].head(10)\n","\n","# Sum the total grams of pbp in a new column\n","df['grams_from_pbp'] = df[pbp_columns].sum(axis=1)\n","print(df.head())\n","\n"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"2vS7it1yQn1N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742692729916,"user_tz":240,"elapsed":56,"user":{"displayName":"Christopher Donovan","userId":"08763921350367890499"}},"outputId":"1a431d38-22f5-4abd-d5ae-965ac5f1374f"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 19386 entries, 0 to 19385\n","Data columns (total 48 columns):\n"," #   Column                     Non-Null Count  Dtype   \n","---  ------                     --------------  -----   \n"," 0   SEQN                       19386 non-null  int64   \n"," 1   weight_2d                  19386 non-null  float64 \n"," 2   grams                      19386 non-null  float64 \n"," 3   f_total_(cup_eq)           19386 non-null  float64 \n"," 4   f_citmlb_(cup_eq)          19386 non-null  float64 \n"," 5   f_other_(cup_eq)           19386 non-null  float64 \n"," 6   f_juice_(cup_eq)           19386 non-null  float64 \n"," 7   v_total_(cup_eq)           19386 non-null  float64 \n"," 8   v_drkgr_(cup_eq)           19386 non-null  float64 \n"," 9   v_redor_total_(cup_eq)     19386 non-null  float64 \n"," 10  v_redor_tomato_(cup_eq)    19386 non-null  float64 \n"," 11  v_redor_other_(cup_eq)     19386 non-null  float64 \n"," 12  v_starchy_total_(cup_eq)   19386 non-null  float64 \n"," 13  v_starchy_potato_(cup_eq)  19386 non-null  float64 \n"," 14  v_starchy_other_(cup_eq)   19386 non-null  float64 \n"," 15  v_other_(cup_eq)           19386 non-null  float64 \n"," 16  v_legumes_(cup_eq)         19386 non-null  float64 \n"," 17  g_total_(oz_eq)            19386 non-null  float64 \n"," 18  g_whole_(oz_eq)            19386 non-null  float64 \n"," 19  g_refined_(oz_eq)          19386 non-null  float64 \n"," 20  pf_total_(oz_eq)           19386 non-null  float64 \n"," 21  pf_mps_total_(oz_eq)       19386 non-null  float64 \n"," 22  pf_meat_(oz_eq)            19386 non-null  float64 \n"," 23  pf_curedmeat_(oz_eq)       19386 non-null  float64 \n"," 24  pf_organ_(oz_eq)           19386 non-null  float64 \n"," 25  pf_poult_(oz_eq)           19386 non-null  float64 \n"," 26  pf_seafd_hi_(oz_eq)        19386 non-null  float64 \n"," 27  pf_seafd_low_(oz_eq)       19386 non-null  float64 \n"," 28  pf_eggs_(oz_eq)            19386 non-null  float64 \n"," 29  pf_soy_(oz_eq)             19386 non-null  float64 \n"," 30  pf_nutsds_(oz_eq)          19386 non-null  float64 \n"," 31  pf_legumes_(oz_eq)         19386 non-null  float64 \n"," 32  d_total_(cup_eq)           19386 non-null  float64 \n"," 33  d_milk_(cup_eq)            19386 non-null  float64 \n"," 34  d_yogurt_(cup_eq)          19386 non-null  float64 \n"," 35  d_cheese_(cup_eq)          19386 non-null  float64 \n"," 36  oils_(grams)               19386 non-null  float64 \n"," 37  solid_fats_(grams)         19386 non-null  float64 \n"," 38  add_sugars_(tsp_eq)        19386 non-null  float64 \n"," 39  a_drinks_(no._of_drinks)   19386 non-null  float64 \n"," 40  gender                     19386 non-null  object  \n"," 41  age                        19386 non-null  float64 \n"," 42  race                       19386 non-null  object  \n"," 43  education                  19386 non-null  object  \n"," 44  income_ratio               17129 non-null  float64 \n"," 45  income_ratio_qs            17129 non-null  category\n"," 46  has_pbp                    19386 non-null  bool    \n"," 47  grams_from_pbp             19386 non-null  float64 \n","dtypes: bool(1), category(1), float64(42), int64(1), object(3)\n","memory usage: 6.8+ MB\n"]}],"source":["df.info()"]},{"cell_type":"markdown","metadata":{"id":"X3RCs3bkRitx"},"source":["Let's also save this as a csv so we can play around with it elsewhere:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uPAMd1gzRl4j"},"outputs":[],"source":["df.to_csv('data/clean/df_nhanes_2017_2023.csv')"]},{"cell_type":"markdown","metadata":{"id":"btI51yPqQn1N"},"source":["## Graphs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TnARSDeTQn1N"},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"qU5GfkAnS2_0"},"source":["Set a common theme for our plots:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Yb-9_GfS15W"},"outputs":[],"source":["sns.set_theme(\n","    style=\"ticks\",\n","    rc= {\n","      \"axes.spines.right\": False,\n","      \"axes.spines.top\": False,\n","      \"figure.figsize\": (6, 6)\n","    }\n","  )"]},{"cell_type":"markdown","metadata":{"id":"GKMzr-WTQn1N"},"source":["PBP consumption by gender. Note that we are using the 2-day weights"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mavLEYpmQn1N"},"outputs":[],"source":["# pbp consumption by gender\n","ax = sns.barplot(\n","  data=df,\n","  y='grams_from_pbp',\n","  x='education',\n","  hue='gender',\n","  order=[\n","    \"Don't know\",\n","    'Less than 9th grade',\n","    'High school/GED',\n","    'Some college or AA',\n","    'College graduate or above'\n","  ],\n","  weights='weight_2d',\n","  errorbar=('ci', 95)\n",")\n","ax.set(\n","  ylabel = 'Proportion of grams from PBP',\n","  xlabel = 'Education',\n","  title = 'PBP Consumption by Education and Gender'\n",")\n","plt.xticks(rotation=45)\n","\n","# Save plot\n","plt.tight_layout()\n","plt.savefig('outputs/checkin_1/pbp_consumption_by_education.png')\n","\n","plt.show()\n","# Would like to adjust names horizontally to line up better after rotation, hjust arg?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hqw_DZFLQn1O"},"outputs":[],"source":["# pbp consumption by race\n","ax = sns.barplot(\n","  data=df,\n","  y='grams_from_pbp',\n","  x='race',\n","  hue='gender',\n","  weights='weight_2d',\n","  errorbar=('ci', 95)\n",")\n","ax.set(\n","  xlabel = 'Race',\n","  ylabel = 'Percentage of grams from PBP',\n","  title = 'PBP as Proportion of Consumption by Race and Gender'\n",")\n","plt.xticks(rotation=45)\n","plt.tight_layout()\n","\n","# Save it\n","plt.savefig('outputs/checkin_1/pbp_consumption_by_race.png')\n","\n","# Show it\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WUhU4I8QQn1O"},"outputs":[],"source":["# pbp consumption by poverty ratio\n","ax = sns.barplot(\n","  data = df,\n","  y='grams_from_pbp',\n","  x='income_ratio_qs',\n","  hue='gender',\n","  weights='weight_2d',\n","  errorbar=('ci', 95)\n",")\n","ax.set(\n","  xlabel = 'Quartiles of Income to Poverty Ratio',\n","  ylabel = 'Percentage of grams from PBP',\n","  title = 'PBP as Proportion of Consumption by Income and Gender'\n",")\n","plt.xticks(rotation=45)\n","plt.tight_layout()\n","\n","# Save it\n","plt.savefig('outputs/checkin_1/pbp_consumption_by_income.png')\n","\n","# Show it\n","plt.show()"]},{"cell_type":"markdown","source":["## Test a Table\n","\n","Just figuring out how to make a LaTeX table"],"metadata":{"id":"fHkY0IWUcAlQ"}},{"cell_type":"code","source":["# Make a smaller DF to play around with\n","small_df = df[['weight_2d', 'grams', 'gender', 'grams_from_pbp']].head()\n","print(small_df)"],"metadata":{"id":"CL44SOMZdjoZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Rename columns to ditch underscores\n","small_df.columns = ['2 Day Weight', 'Total Grams Over 2 Days', 'Gender', 'Total PBP Grams']\n","print(small_df)"],"metadata":{"id":"WR950CSLkh9k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["small_df.to_latex(\n","  'outputs/checkin_1/test_table.tex',\n","  index=False,\n","  float_format=\"%.2f\",\n","  label='test_table',\n","  caption='This is a test table',\n","  position='h'\n",")\n"],"metadata":{"id":"Mmc44_yFcGey"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Looks like this mostly works. Could use some proper formatting though"],"metadata":{"id":"bXAVtmbMcGmn"}}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.0"}},"nbformat":4,"nbformat_minor":0}